{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Challenge\n",
    "Goal: to capture the complexity and nuances around the evolution of the pandemic at various stages and locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consider the following settings:\n",
    "1. *Timepoint 1*: May 1st, 2020. Setting: Michigan State at the beginning of the pandemic when masking was the main preventative measure. No vaccines available.\n",
    "2. *Timepoint 2*: May 1st, 2021. Setting: Michigan State prior to the arrival of the Delta variant. Vaccines available.\n",
    "3. *Timepoint 3*: December 15th, 2021. Setting: Michigan State during the start of the first Omicron wave.\n",
    "\n",
    "4. *BONUS*: Consider the same three time points, but change the setting to Louisiana, which had different COVID-19 dynamics compared to the Northern and Northeastern states.\n",
    "\n",
    "## ...and related questions for each:\n",
    "1. What is the most relevant data to use for model calibration?\n",
    "2. What was our understanding of COVID-19 viral mechanisms at the time? For example, early in the pandemic, we didn't know if reinfection was a common occurance, or even possible.\n",
    "3. What are the parameters related to contagiousness/transmissibility and severity of the dominant strain at the time?\n",
    "4. What policies were in place for a stated location, and how can this information be incorporated into models? (See https://www.bsg.ox.ac.uk/research/covid-19-government-response-tracker for time series of interventions.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each setting:\n",
    "1. (a) Take a single model, calibrate it using historical data prior to the given date, and create a 4-week forecast for cases, hospitalizations, and deaths beginning on the given date. (b) Evaluate the forecast using the COVID-19 Forecasting Hub Error Metrics (WIS, MAE). The single model evaluation should be done in the same way as the ensemble.\n",
    "\n",
    "2. Repeat (1), but with an ensemble of different models.\n",
    "\n",
    "    a. It is fine to calibrate each model independently and weight naively.\n",
    "    \n",
    "    b. It would also be fine to calibrate the ensemble as a whole, assigning weights to the different component models, so that you minimize the error of the ensemble vs. historical data.\n",
    "    \n",
    "    c. Use the calibration scores and error metrics computed by the CDC Forecasting Hub. As stated on their [website](https://covid19forecasthub.org/doc/reports/): \n",
    "    \n",
    "    “Periodically, we evaluate the accuracy and precision of the [ensemble forecast](https://covid19forecasthub.org/doc/ensemble/) and component models over recent and historical forecasting periods. Models forecasting incident hospitalizations at a national and state level are evaluated using [adjusted relative weighted interval scores (WIS, a measure of distributional accuracy)](https://arxiv.org/abs/2005.12881), and adjusted relative mean absolute error (MAE), and calibration scores. Scores are evaluated across weeks, locations, and targets. You can read [a paper explaining these procedures in more detail](https://www.medrxiv.org/content/10.1101/2021.02.03.21250974v1), and look at [the most recent monthly evaluation reports](https://covid19forecasthub.org/eval-reports). The final report that includes case and death forecast evaluations is 2023-03-13.” \n",
    "\n",
    "3. Produce the forecast outputs in the format specified by the CDC forecasting challenge, including the specified quantiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Use the following data sources:\n",
    "1. Cases: [Johns Hopkins](https://github.com/CSSEGISandData/COVID-19/blob/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv), [Reich Lab](https://github.com/reichlab/covid19-forecast-hub/blob/master/data-truth/truth-Incident%20Cases.csv) (pulled from Johns Hopkins, but formatted)\n",
    "\n",
    "2. Hospitalizations: [HealthData.gov](https://healthdata.gov/Hospital/COVID-19-Reported-Patient-Impact-and-Hospital-Capa/g62h-syeh)\n",
    "\n",
    "3. Deaths: [Johns Hopkins](https://github.com/reichlab/covid19-forecast-hub/blob/master/data-truth/truth-Incident%20Deaths.csv), [Reich Lab](https://github.com/reichlab/covid19-forecast-hub/blob/master/data-truth/truth-Cumulative%20Deaths.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dependencies (you'll have numpy and pandas too)\n",
    "from pyciemss.utils.toronto_hackathon_utils.toronto_ensemble_challenge_utils import get_case_hosp_death_data\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the region of interest and infectious period, get the DataFrame for that region\n",
    "US_region = \"MI\" # 2-letter state abbreviation string (or \"US\")\n",
    "infectious_period = 7 # duration of infectious period (in days)\n",
    "plot_data = True # plot the data when true\n",
    "\n",
    "data = get_case_hosp_death_data(US_region = US_region, infectious_period = infectious_period, make_csv=False)\n",
    "# Note: source datasets are quite large, so this will take a minute to run\n",
    "\n",
    "if plot_data:\n",
    "    # Plot case census data\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(data.index, data[\"case_census\"], 'o')\n",
    "    plt.title(\"Case Census\")\n",
    "\n",
    "    # Plot hosp census data\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(data.index, data[\"hosp_census\"], 'o')\n",
    "    plt.title(\"Hospital Census\")\n",
    "\n",
    "    # Plot cumulative deaths\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(data.index, data[\"cumulative_deaths\"], 'o')\n",
    "    plt.title(\"Cumulative Deaths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models:\n",
    "1. You may consider any of the models you have seen in the started kit, or 6-month hackathon and evaluation scenarios.\n",
    "\n",
    "2. You may search for new models in the literature, or use TA2 model extension/transformation capabilities to modify models already in Terarium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
