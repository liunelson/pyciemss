{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Challenge\n",
    "Goal: to capture the complexity and nuances around the evolution of the pandemic at various stages and locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consider the following settings:\n",
    "1. *Timepoint 1*: May 1st, 2020. Setting: Michigan State at the beginning of the pandemic when masking was the main preventative measure. No vaccines available.\n",
    "2. *Timepoint 2*: May 1st, 2021. Setting: Michigan State prior to the arrival of the Delta variant. Vaccines available.\n",
    "3. *Timepoint 3*: December 15th, 2021. Setting: Michigan State during the start of the first Omicron wave.\n",
    "\n",
    "4. *BONUS*: Consider the same three time points, but change the setting to Louisiana, which had different COVID-19 dynamics compared to the Northern and Northeastern states.\n",
    "\n",
    "## ...and related questions for each:\n",
    "1. What is the most relevant data to use for model calibration?\n",
    "2. What was our understanding of COVID-19 viral mechanisms at the time? For example, early in the pandemic, we didn't know if reinfection was a common occurance, or even possible.\n",
    "3. What are the parameters related to contagiousness/transmissibility and severity of the dominant strain at the time?\n",
    "4. What policies were in place for a stated location, and how can this information be incorporated into models? (See https://www.bsg.ox.ac.uk/research/covid-19-government-response-tracker for time series of interventions.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each setting:\n",
    "1. (a) Take a single model, calibrate it using historical data prior to the given date, and create a 4-week forecast for cases, hospitalizations, and deaths beginning on the given date. (b) Evaluate the forecast using the COVID-19 Forecasting Hub Error Metrics (WIS, MAE). The single model evaluation should be done in the same way as the ensemble.\n",
    "\n",
    "2. Repeat (1), but with an ensemble of different models.\n",
    "\n",
    "    a. It is fine to calibrate each model independently and weight naively.\n",
    "    \n",
    "    b. It would also be fine to calibrate the ensemble as a whole, assigning weights to the different component models, so that you minimize the error of the ensemble vs. historical data.\n",
    "    \n",
    "    c. Use the calibration scores and error metrics computed by the CDC Forecasting Hub. As stated on their [website](https://covid19forecasthub.org/doc/reports/): \n",
    "    \n",
    "    “Periodically, we evaluate the accuracy and precision of the [ensemble forecast](https://covid19forecasthub.org/doc/ensemble/) and component models over recent and historical forecasting periods. Models forecasting incident hospitalizations at a national and state level are evaluated using [adjusted relative weighted interval scores (WIS, a measure of distributional accuracy)](https://arxiv.org/abs/2005.12881), and adjusted relative mean absolute error (MAE), and calibration scores. Scores are evaluated across weeks, locations, and targets. You can read [a paper explaining these procedures in more detail](https://www.medrxiv.org/content/10.1101/2021.02.03.21250974v1), and look at [the most recent monthly evaluation reports](https://covid19forecasthub.org/eval-reports). The final report that includes case and death forecast evaluations is 2023-03-13.” \n",
    "\n",
    "3. Produce the forecast outputs in the format specified by the CDC forecasting challenge, including the specified quantiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Use the following data sources:\n",
    "1. Cases: [Johns Hopkins](https://github.com/CSSEGISandData/COVID-19/blob/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv), [Reich Lab](https://github.com/reichlab/covid19-forecast-hub/blob/master/data-truth/truth-Incident%20Cases.csv) (pulled from Johns Hopkins, but formatted)\n",
    "\n",
    "2. Hospitalizations: [HealthData.gov](https://healthdata.gov/Hospital/COVID-19-Reported-Patient-Impact-and-Hospital-Capa/g62h-syeh)\n",
    "\n",
    "3. Deaths: [Johns Hopkins](https://github.com/reichlab/covid19-forecast-hub/blob/master/data-truth/truth-Incident%20Deaths.csv), [Reich Lab](https://github.com/reichlab/covid19-forecast-hub/blob/master/data-truth/truth-Cumulative%20Deaths.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dependencies and functions from utils file\n",
    "from pyciemss.utils.toronto_hackathon_utils.toronto_ensemble_challenge_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the region of interest and infectious period, get the DataFrame containing case and hospital census data, and death data for that region, and plot said data if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/altu809/Projects/pyciemss/src/pyciemss/utils/toronto_hackathon_utils/toronto_ensemble_challenge_utils.py:27: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  raw_cases = pd.read_csv(url)\n",
      "/Users/altu809/Projects/pyciemss/src/pyciemss/utils/toronto_hackathon_utils/toronto_ensemble_challenge_utils.py:28: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  raw_cases['date'] = pd.to_datetime(raw_cases.date, infer_datetime_format=True)\n",
      "/Users/altu809/Projects/pyciemss/src/pyciemss/utils/toronto_hackathon_utils/toronto_ensemble_challenge_utils.py:34: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  raw_hosp['date'] = pd.to_datetime(raw_hosp.date, infer_datetime_format=True)\n",
      "/Users/altu809/Projects/pyciemss/src/pyciemss/utils/toronto_hackathon_utils/toronto_ensemble_challenge_utils.py:39: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  raw_deaths = pd.read_csv(url)\n",
      "/Users/altu809/Projects/pyciemss/src/pyciemss/utils/toronto_hackathon_utils/toronto_ensemble_challenge_utils.py:40: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  raw_deaths['date'] = pd.to_datetime(raw_deaths.date, infer_datetime_format=True)\n",
      "/Users/altu809/Projects/pyciemss/src/pyciemss/utils/toronto_hackathon_utils/toronto_ensemble_challenge_utils.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  regional_cases[\"case census\"] = 0\n"
     ]
    }
   ],
   "source": [
    "# Declare the region of interest and infectious period, get the DataFrame for that region\n",
    "US_region = \"MI\" # 2-letter state abbreviation string (or \"US\")\n",
    "regional_population = 10050000 # Michigan: 10,050,000 / Louisiana: 4,624,000\n",
    "infectious_period = 7 # duration of infectious period (in days)\n",
    "plot_data = False # plot the data when true\n",
    "\n",
    "# Note: source datasets are quite large, so this will take a minute to run\n",
    "data = get_case_hosp_death_data(US_region = US_region, infectious_period = infectious_period, make_csv=False)\n",
    "data = data.reset_index()\n",
    "# print(data)\n",
    "\n",
    "# FYI: hosp data starts around 07/14/2020 and is NaN before, case and death data ends 03/04/2023 and is NaN after\n",
    "\n",
    "if plot_data:\n",
    "    # Plot case census data\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(data.index, data[\"case_census\"], 'o')\n",
    "    plt.title(\"Case Census\")\n",
    "\n",
    "    # Plot hosp census data\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(data.index, data[\"hosp_census\"], 'o')\n",
    "    plt.title(\"Hospital Census\")\n",
    "\n",
    "    # Plot cumulative deaths\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(data.index, data[\"cumulative_deaths\"], 'o')\n",
    "    plt.title(\"Cumulative Deaths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set relevant dates, test and train intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set train start date at the 55th day of data to use most/best historic data available\n",
    "train_start_date = str(data[\"date\"][55]) # this is 03/17/2020\n",
    "\n",
    "# Given timepoints will act as test start dates\n",
    "timepoint1 = \"2020-05-01\" \n",
    "timepoint2 = \"2021-05-01\"\n",
    "timepoint3 = \"2021-12-15\"\n",
    "\n",
    "# Set test end dates 4 weeks after timepoints\n",
    "test_end_date1 = \"2020-05-29\"\n",
    "test_end_date2 = \"2021-05-29\"\n",
    "test_end_date3 = \"2022-01-12\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather train and test data corresponding to Timepoint 1\n",
    "train_data1, train_cases1, train_timepoints1, test_cases1, test_timepoints1, all_timepoints1 = \\\n",
    "get_train_test_data(data, train_start_date, timepoint1, test_end_date1)\n",
    "data_file1 = US_region + \"_train_data_1.csv\"\n",
    "train_data_to_csv(train_data1, data_file1)\n",
    "\n",
    "# Set the start time (for all simulations)\n",
    "start_time = train_timepoints1[0] - 1e-5\n",
    "\n",
    "# Gather train and test data corresponding to Timepoint 2\n",
    "train_data2, train_cases2, train_timepoints2, test_cases2, test_timepoints2, all_timepoints2 = \\\n",
    "get_train_test_data(data, train_start_date, timepoint2, test_end_date2)\n",
    "data_file2 = US_region + \"_train_data_2.csv\"\n",
    "train_data_to_csv(train_data2, data_file2)\n",
    "\n",
    "# Gather train and test data corresponding to Timepoint 3\n",
    "train_data3, train_cases3, train_timepoints3, test_cases3, test_timepoints3, all_timepoints3 = \\\n",
    "get_train_test_data(data, train_start_date, timepoint3, test_end_date3)\n",
    "data_file3 = US_region + \"_train_data_3.csv\"\n",
    "train_data_to_csv(train_data2, data_file3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models:\n",
    "1. You may consider any of the models you have seen in the started kit, or 6-month hackathon and evaluation scenarios.\n",
    "\n",
    "2. You may search for new models in the literature, or use TA2 model extension/transformation capabilities to modify models already in Terarium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dependencies for ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ensembling dependencies\n",
    "import os\n",
    "from pyciemss.PetriNetODE.interfaces import load_petri_model\n",
    "from pyciemss.Ensemble.interfaces import load_and_sample_petri_ensemble, load_and_calibrate_and_sample_ensemble_model\n",
    "from pyciemss.visuals import plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get models to be ensembled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Model file not found: scenario1_a.json",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# FIRST_PATH = \"../Examples_for_TA2_Model_Representation/\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# # Model 1\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Model 1\u001b[39;00m\n\u001b[1;32m     23\u001b[0m filename1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscenario1_a.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 24\u001b[0m model1 \u001b[38;5;241m=\u001b[39m \u001b[43mload_petri_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_uncertainty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Model 2\u001b[39;00m\n\u001b[1;32m     27\u001b[0m filename2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscenario1_c.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Projects/pyciemss/src/pyciemss/PetriNetODE/interfaces.py:614\u001b[0m, in \u001b[0;36mload_petri_model\u001b[0;34m(petri_model_or_path, add_uncertainty, pseudocount, compile_rate_law_p)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;124;03mLoad a petri net from a file and compile it into a probabilistic program.\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m add_uncertainty:\n\u001b[0;32m--> 614\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mScaledBetaNoisePetriNetODESystem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_askenet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpetri_model_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_rate_law_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompile_rate_law_p\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m     model\u001b[38;5;241m.\u001b[39mpseudocount \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(pseudocount)\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/functools.py:926\u001b[0m, in \u001b[0;36msingledispatchmethod.__get__.<locals>._method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_method\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    925\u001b[0m     method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatcher\u001b[38;5;241m.\u001b[39mdispatch(args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m--> 926\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/pyciemss/src/pyciemss/PetriNetODE/base.py:441\u001b[0m, in \u001b[0;36mMiraPetriNetODESystem._from_path\u001b[0;34m(cls, model_json_path, **kwargs)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_json_path):\n\u001b[0;32m--> 441\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel file not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_json_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(model_json_path) \u001b[38;5;28;01mas\u001b[39;00m fh:\n\u001b[1;32m    443\u001b[0m         model_json \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(fh)\n",
      "\u001b[0;31mValueError\u001b[0m: Model file not found: scenario1_a.json"
     ]
    }
   ],
   "source": [
    "# FIRST_PATH = \"../Examples_for_TA2_Model_Representation/\"\n",
    "\n",
    "# # Model 1\n",
    "# filename1 = \"SEIARHDS_AMR.json\"\n",
    "# filename1 = os.path.join(FIRST_PATH, filename1)\n",
    "# model1 = load_petri_model(filename1, add_uncertainty=True)\n",
    "\n",
    "# # Model 2\n",
    "# filename2 = \"SEIARHD_AMR.json\"\n",
    "# filename2 = os.path.join(FIRST_PATH, filename2)\n",
    "# # model2 = load_petri_model(filename2, add_uncertainty=True)\n",
    "\n",
    "# # Model 3\n",
    "# filename3 = \"SIRHD_AMR.json\"\n",
    "# filename3 = os.path.join(FIRST_PATH, filename3)\n",
    "# # model3 = load_petri_model(filename3, add_uncertainty=True)\n",
    "\n",
    "# model_paths = [filename1, filename2, filename3]\n",
    "\n",
    "##### Using hackathon models\n",
    "\n",
    "# Model 1\n",
    "filename1 = \"scenario1_a.json\"\n",
    "model1 = load_petri_model(filename1, add_uncertainty=True)\n",
    "\n",
    "# Model 2\n",
    "filename2 = \"scenario1_c.json\"\n",
    "# model2 = load_petri_model(filename2, add_uncertainty=True)\n",
    "\n",
    "# Model 3\n",
    "filename3 = \"scenario2_a_beta_scale_static.json\"\n",
    "# model3 = load_petri_model(filename3, add_uncertainty=True)\n",
    "\n",
    "model_paths = [filename1, filename2, filename3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create functions to define solution mapping dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define type of solution mapping required by each model\n",
    "def solution_mapping1(model_solution: dict) -> dict:\n",
    "    # solution mapping for model 1: SEIARHDS\n",
    "    mapped_dict = {}\n",
    "    mapped_dict[\"Cases\"] = model_solution[\"symptomatic_population\"] + model_solution[\"asymptomatic_population\"]\n",
    "    mapped_dict[\"Hospitalizations\"] = model_solution[\"hospitalized_population\"]\n",
    "    mapped_dict[\"Deaths\"] = model_solution[\"deceased_population\"]\n",
    "    return mapped_dict\n",
    "\n",
    "def solution_mapping2(model_solution: dict) -> dict:\n",
    "    # solution mapping for model 2: SEIARHD\n",
    "    mapped_dict = {}\n",
    "    mapped_dict[\"Cases\"] = model_solution[\"symptomatic_population\"] + model_solution[\"asymptomatic_population\"]\n",
    "    mapped_dict[\"Hospitalizations\"] = model_solution[\"hospitalized_population\"]\n",
    "    mapped_dict[\"Deaths\"] = model_solution[\"deceased_population\"]\n",
    "    return mapped_dict\n",
    "\n",
    "def solution_mapping3(model_solution: dict) -> dict:\n",
    "    # solution mapping for model 3: SIRHD\n",
    "    mapped_dict = {}\n",
    "    mapped_dict[\"Cases\"] = model_solution[\"infectious_population\"]\n",
    "    mapped_dict[\"Hospitalizations\"] = model_solution[\"hospitalized_population\"]\n",
    "    mapped_dict[\"Deaths\"] = model_solution[\"deceased_population\"]\n",
    "    return mapped_dict\n",
    "\n",
    "solution_mappings = [solution_mapping1, solution_mapping2, solution_mapping3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create start states for each model at designated time points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define start states for each model\n",
    "def create_start_state1(data, t_0, regional_population):\n",
    "    '''Create the start state for Model 1 from data using our best guesses for\n",
    "    mapping from observed variables to model state variables.'''\n",
    "    \n",
    "    start_state = data.set_index('date').loc[t_0].to_dict()\n",
    "    returned_state = {}\n",
    "    returned_state[\"exposed_population\"] = start_state['case_census'] / 2\n",
    "    if start_state['case_census'] <= 0:\n",
    "        returned_state[\"symptomatic_population\"] = 1\n",
    "    else:\n",
    "        returned_state[\"symptomatic_population\"] = start_state['case_census'] / 4\n",
    "    returned_state[\"asymptomatic_population\"] = 3 * start_state['case_census'] / 4\n",
    "    returned_state[\"recovered_population\"] = 2 * start_state['case_census']\n",
    "    \n",
    "    if start_state[\"hosp_census\"] > 0:\n",
    "        returned_state[\"hospitalized_population\"] = start_state[\"hosp_census\"]\n",
    "    else:\n",
    "        returned_state[\"hospitalized_population\"] = 0\n",
    "    \n",
    "    returned_state[\"deceased_population\"] = start_state[\"cumulative_deaths\"]\n",
    "    returned_state[\"susceptible_population\"] = regional_population - sum(returned_state.values())\n",
    "    \n",
    "    assert(returned_state[\"susceptible_population\"] > 0)\n",
    "    return returned_state #{k:v/regional_population for k, v in returned_state.items()}\n",
    "\n",
    "def create_start_state2(data, t_0, regional_population):\n",
    "    '''Create the start state for Model 2 from data using our best guesses for\n",
    "    mapping from observed variables to model state variables.'''\n",
    "    \n",
    "    start_state = data.set_index('date').loc[t_0].to_dict()\n",
    "    returned_state = {}\n",
    "    returned_state[\"exposed_population\"] = start_state['case_census'] / 2\n",
    "    if start_state['case_census'] <= 0:\n",
    "        returned_state[\"symptomatic_population\"] = 1\n",
    "    else:\n",
    "        returned_state[\"symptomatic_population\"] = start_state['case_census'] / 4\n",
    "    returned_state[\"asymptomatic_population\"] = 3 * start_state['case_census'] / 4\n",
    "    returned_state[\"recovered_population\"] = 2 * start_state['case_census']\n",
    "    \n",
    "    if start_state[\"hosp_census\"] > 0:\n",
    "        returned_state[\"hospitalized_population\"] = start_state[\"hosp_census\"]\n",
    "    else:\n",
    "        returned_state[\"hospitalized_population\"] = 0\n",
    "    \n",
    "    returned_state[\"deceased_population\"] = start_state[\"cumulative_deaths\"]\n",
    "    returned_state[\"susceptible_population\"] = regional_population - sum(returned_state.values())\n",
    "    \n",
    "    assert(returned_state[\"susceptible_population\"] > 0)\n",
    "    return returned_state #{k:v/regional_population for k, v in returned_state.items()}\n",
    "\n",
    "def create_start_state3(data, t_0, regional_population):\n",
    "    '''Create the start state for Model 3 from data using our best guesses for\n",
    "    mapping from observed variables to model state variables.'''\n",
    "    \n",
    "    start_state = data.set_index('date').loc[t_0].to_dict()\n",
    "    returned_state = {}\n",
    "    if start_state['case_census'] <= 0:\n",
    "        returned_state[\"infectious_population\"] = 1\n",
    "    else:\n",
    "        returned_state[\"infectious_population\"] = start_state['case_census']\n",
    "    returned_state[\"recovered_population\"] = 2 * start_state['case_census']\n",
    "    \n",
    "    if start_state[\"hosp_census\"] > 0:\n",
    "        returned_state[\"hospitalized_population\"] = start_state[\"hosp_census\"]\n",
    "    else:\n",
    "        returned_state[\"hospitalized_population\"] = 0\n",
    "    \n",
    "    returned_state[\"deceased_population\"] = start_state[\"cumulative_deaths\"]\n",
    "    returned_state[\"susceptible_population\"] = regional_population - sum(returned_state.values())\n",
    "    \n",
    "    assert(returned_state[\"susceptible_population\"] > 0)\n",
    "    return returned_state #{k:v/regional_population for k, v in returned_state.items()}\n",
    "\n",
    "start_states = [create_start_state1(data, train_start_date, regional_population), \n",
    "               create_start_state2(data, train_start_date, regional_population), \n",
    "               create_start_state3(data, train_start_date, regional_population)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up and calibrate the ensemble model to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = [1] #[0.5, 0.5]\n",
    "# num_samples = 100\n",
    "# timepoints = all_timepoints1 #[0.0, 1.0, 2.0, 3.0, 4.0]\n",
    "\n",
    "# # Run sampling\n",
    "# ensemble_samples = load_and_sample_petri_ensemble(\n",
    "#     [model_paths[0]], weights, [solution_mapping1], num_samples, timepoints\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../hackathon_prep/\"\n",
    "data_filename = data_file1\n",
    "data_path = os.path.join(DATA_PATH, data_filename)\n",
    "weights = [] # assume equal weights initially\n",
    "for model in model_paths:\n",
    "    weights.append(1/len(model_paths))\n",
    "num_samples = 100\n",
    "num_iterations = 300\n",
    "all_timepoints = all_timepoints1\n",
    "\n",
    "# Run the calibration and sampling\n",
    "calibrated_samples = load_and_calibrate_and_sample_ensemble_model(\n",
    "    model_paths,\n",
    "    data_path,\n",
    "    weights,\n",
    "    solution_mappings,\n",
    "    num_samples,\n",
    "    timepoints=all_timepoints,\n",
    "    start_states=start_states,\n",
    "    total_population=regional_population,\n",
    "    verbose=True,\n",
    "    num_iterations=num_iterations,\n",
    "    time_unit=\"days\",\n",
    ")\n",
    "\n",
    "# Save results\n",
    "# result[\"data\"].to_csv(\n",
    "#     os.path.join(DEMO_PATH, \"results_petri_ensemble/calibrated_sample_results.csv\"), index=False\n",
    "# )\n",
    "\n",
    "plots.ipy_display(result[\"visual\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = [1] #[0.5, 0.5]\n",
    "# num_samples = 100\n",
    "# timepoints = all_timepoints1 #[0.0, 1.0, 2.0, 3.0, 4.0]\n",
    "\n",
    "# # Run sampling\n",
    "# ensemble_samples = load_and_sample_petri_ensemble(\n",
    "#     [model_paths[0]], weights, [solution_mapping1], num_samples, timepoints\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../hackathon_prep/\"\n",
    "data_filename = data_file1\n",
    "data_path = os.path.join(DATA_PATH, data_filename)\n",
    "weights = [1] # assume equal weights initially\n",
    "num_samples = 100\n",
    "num_iterations = 100\n",
    "all_timepoints = all_timepoints1\n",
    "\n",
    "# Run the calibration and sampling\n",
    "calibrated_samples = load_and_calibrate_and_sample_ensemble_model(\n",
    "    [model_paths[0]],\n",
    "    data_path,\n",
    "    weights,\n",
    "    [solution_mapping1],\n",
    "    num_samples,\n",
    "    all_timepoints,\n",
    "#     start_states=start_states[0],\n",
    "    total_population=regional_population,\n",
    "    verbose=True,\n",
    "    num_iterations=num_iterations,\n",
    "    time_unit=\"days\",\n",
    "    visual_options={\"title\": \"Calibrated Ensemble\", \"subset\":\".*_sol\"},\n",
    "    lr = 0.02,\n",
    "    method=\"euler\",\n",
    ")\n",
    "\n",
    "# Save results\n",
    "# result[\"data\"].to_csv(\n",
    "#     os.path.join(DEMO_PATH, \"results_petri_ensemble/calibrated_sample_results.csv\"), index=False\n",
    "# )\n",
    "\n",
    "plots.ipy_display(result[\"visual\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the calibration and sampling\n",
    "calibrated_samples = load_and_calibrate_and_sample_petri_model(petri_model_or_path=SEIRD_model_path,\n",
    "                                                               data_path=\"mapped_data.csv\",\n",
    "                                                               num_samples=num_samples,\\\n",
    "    timepoints=mapped_data[\"timepoints\"],verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_timepoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_and_sample_petri_ensemble(\n",
    "    model_paths,\n",
    "    weights,\n",
    "    solution_mappings,\n",
    "    num_samples,\n",
    "    test_timepoints,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
